{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://ieeexplore.ieee.org/document/9627980 (Paper URL)\n",
        "\n",
        "## COMMANDS TO RUN ##\n",
        "# !git clone https://github.com/airtlab/A-Dataset-for-Automatic-Violence-Detection-in-Videos\n",
        "# !mv /content/A-Dataset-for-Automatic-Violence-Detection-in-Videos /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "Ur7E5gXvSflw"
      },
      "id": "Ur7E5gXvSflw",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ca21d804",
      "metadata": {
        "id": "ca21d804"
      },
      "source": [
        "# Violence Detection using AirtLab Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "001c797b",
      "metadata": {
        "id": "001c797b"
      },
      "source": [
        "## Preparations and setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQO7KG48KJ2T",
        "outputId": "81dde2d3-25fd-4fd7-939d-3c61258a5727"
      },
      "id": "wQO7KG48KJ2T",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "909d89b7",
      "metadata": {
        "id": "909d89b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7584c0-cc67-4e21-fefd-319f23e82fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing Libraries...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Can't find torchcodec, installing...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h    Successfully installed torchcodec!\n",
            "  Can't find torchmetrics, installing...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h    Successfully installed torchmetrics!\n",
            "  Initializing SummaryWriter..\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "print(\"Importing Libraries...\")\n",
        "!pip install av -q\n",
        "## Numerical Libraries ##\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "## Visualization Libraries ##\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## Random ##\n",
        "import random\n",
        "\n",
        "## PyTorch Modules ##\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "## TorchVision Modules ##\n",
        "from torchvision.io import VideoReader\n",
        "from torchvision.transforms.transforms import Resize\n",
        "\n",
        "## TorchCodec Modules ##\n",
        "try:\n",
        "    from torchcodec.decoders import VideoDecoder\n",
        "except ModuleNotFoundError:\n",
        "    print(\"  Can't find torchcodec, installing...\")\n",
        "    !pip install torchcodec av -q\n",
        "    from torchcodec.decoders import VideoDecoder\n",
        "    print(\"    Successfully installed torchcodec!\")\n",
        "\n",
        "## TorchMetrics Modules ##\n",
        "try:\n",
        "    from torchmetrics import Accuracy, F1Score, Precision, Recall\n",
        "except ModuleNotFoundError:\n",
        "    print(\"  Can't find torchmetrics, installing...\")\n",
        "    !pip install torchmetrics -q\n",
        "    from torchmetrics import Accuracy, F1Score, Precision, Recall\n",
        "    print(\"    Successfully installed torchmetrics!\")\n",
        "\n",
        "## Typing library for type hints ##\n",
        "from typing import List\n",
        "\n",
        "## Utility libraries ##\n",
        "import os\n",
        "import warnings\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "## Ignore Warnings ##\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "## Initialize TensorBoard for experiment tracking ##\n",
        "print(\"  Initializing SummaryWriter..\")\n",
        "writer = SummaryWriter(\n",
        "    \"./runs/\"\n",
        ")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Device Agnostic\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "U1OGcEHeslay"
      },
      "id": "U1OGcEHeslay",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Utility Functions"
      ],
      "metadata": {
        "id": "S8ulY2jpknGT"
      },
      "id": "S8ulY2jpknGT"
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sizes(model, input_tensor):\n",
        "    output = input_tensor\n",
        "    for m in model.children():\n",
        "        output = m(output)\n",
        "        print(f\"Layer: {m}\\n{output.shape}\\n-------------------------------\")\n",
        "    return output"
      ],
      "metadata": {
        "id": "nXPZFU7GkpDJ"
      },
      "id": "nXPZFU7GkpDJ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a0d77056",
      "metadata": {
        "id": "a0d77056"
      },
      "source": [
        "## Dataset Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d4e936f7",
      "metadata": {
        "id": "d4e936f7"
      },
      "outputs": [],
      "source": [
        "class ViolenceDetectionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Builds the dataset with a lazy-loading approach to avoid huge memory \"leaks\" (eager loading for this dataset will consume huge amounts of memory, although not memory leak in the traditioal sense, still consumes lots of memory.)\n",
        "\n",
        "    Parameters:\n",
        "    video_paths (List[str]); List of paths to access videos\n",
        "    labels (List[int]): List of labels\n",
        "    normalize (bool, default False): Whether to normalize the returned frames or not\n",
        "    \"\"\"\n",
        "    def __init__(self, video_paths:List[str], labels:List[int], normalize:bool=False):\n",
        "        # self.cam1_vid_paths = cam1_vid_paths\n",
        "        # self.cam2_vid_paths = cam2_vid_paths\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.normalize = normalize\n",
        "\n",
        "    @staticmethod\n",
        "    def _read_video(video_path:str, target_num_frames:int=16, frame_rate:int=30, normalize:bool=False) -> List[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Reads video file\n",
        "\n",
        "        Parameters:\n",
        "        video_path (str): The path to the video (can be the relative or absolute path)\n",
        "        target_num_frames (int, default 16): The number of sample frames to extract from the video\n",
        "        frame_rate (int, default 30, DO NOT MODIFY IF USING AIRTLAB DATASET): the videos frame rate (fps)\n",
        "        normalize (bool, defualt False): Whether to normalize the sampled frames or not\n",
        "\n",
        "        Returns:\n",
        "        List[torch.Tensor()]: A list of tensors representing the extracted sample frames from the video\n",
        "        \"\"\"\n",
        "\n",
        "        frames = []\n",
        "        resizer = Resize((112, 112))\n",
        "        video_reader = VideoReader(video_path, 'video')\n",
        "\n",
        "        # Read the video frame by frame\n",
        "        for frame in video_reader:\n",
        "            frames.append(frame['data'])\n",
        "\n",
        "        # Extract information about the video\n",
        "        num_frames = len(frames)\n",
        "        num_seconds = num_frames/frame_rate # Get the length of the video in seconds\n",
        "        sampling_step = round((frame_rate*num_seconds)/target_num_frames) # Get the number of frames to sample per each second\n",
        "\n",
        "        # Sample the video\n",
        "        sampled_frames = frames[::sampling_step]\n",
        "        number_of_samples = len(sampled_frames)\n",
        "        if number_of_samples > target_num_frames: # Sometimes it can take one extra frame making the total number of frames target_num_frames+1\n",
        "            sampled_frames = sampled_frames[:target_num_frames]\n",
        "\n",
        "        elif number_of_samples < target_num_frames: # If the number of frames is less than target_num_frames, then duplicate a random frame\n",
        "            diff = target_num_frames - number_of_samples # Number of frames to duplicate\n",
        "            mid_frame = (number_of_samples//2) # The middle frame, most likely captures an action, so duplicating that (or the one before/after it) will at least be a meaningful duplication.\n",
        "            i = random.choice([mid_frame-1, mid_frame, mid_frame+1]) # Random frame index\n",
        "            frame_to_duplicate = sampled_frames[i]\n",
        "            sampled_frames.extend([frame_to_duplicate] * diff)\n",
        "\n",
        "        # resize the sampled frames (better than resizing all frames for computational reasons)\n",
        "        frames = []\n",
        "        if normalize:\n",
        "            for sample in sampled_frames:\n",
        "                processed_frame = resizer(sample.float()/255.0) # Normalize each frame to be in range [0,1]\n",
        "                frames.append(processed_frame)\n",
        "        else:\n",
        "            for sample in sampled_frames:\n",
        "                processed_frame = resizer(sample.float())\n",
        "                frames.append(processed_frame)\n",
        "        return torch.stack(frames).permute(1, 0, 2, 3).to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # cam1_vid = self._read_video(video_path=self.cam1_vid_paths[idx], normalize=self.normalize)\n",
        "        # cam2_vid = self._read_video(video_path=self.cam2_vid_paths[idx], normalize=self.normalize)\n",
        "        X = self._read_video(video_path=self.video_paths[idx], normalize=self.normalize)\n",
        "        y = torch.Tensor([self.labels[idx]]).to(device)\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e7bdf10",
      "metadata": {
        "id": "4e7bdf10"
      },
      "source": [
        "## Read Videos functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "def38304",
      "metadata": {
        "id": "def38304"
      },
      "outputs": [],
      "source": [
        "# Read Videos\n",
        "def read_nonViolent_videos() -> List[torch.Tensor] | List[torch.Tensor] | List[int]:\n",
        "    \"\"\"\n",
        "    Read the Non-Violent video paths. Mainly for lazy loading in a later Dataset class\n",
        "\n",
        "    Returns:\n",
        "    cam1_vids: Video paths from camera 1\n",
        "    cam2_vids: Video paths from camera 2\n",
        "    label: a List of the labels (0s in this case, signifying non-violent)\n",
        "    \"\"\"\n",
        "    nonViolent_cam1_path = \"/content/drive/MyDrive/Datasets/A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/non-violent/cam1\"\n",
        "    nonViolent_cam2_path = \"/content/drive/MyDrive/Datasets/A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/non-violent/cam2\"\n",
        "    num_nonViolent_videos = 60\n",
        "    cam1_vid_paths = []\n",
        "    cam2_vid_paths = []\n",
        "    for video_num in range(1, num_nonViolent_videos+1):\n",
        "        # Prepare the relative path of the video (for both cameras)\n",
        "        video_index = f\"{video_num}.mp4\"\n",
        "        cam1_video_path = os.path.join(nonViolent_cam1_path,video_index)\n",
        "        cam2_video_path = os.path.join(nonViolent_cam2_path,video_index)\n",
        "\n",
        "        # Append video to list of videos\n",
        "        cam1_vid_paths.append(cam1_video_path)\n",
        "        cam2_vid_paths.append(cam2_video_path)\n",
        "    label = [0]*num_nonViolent_videos*2 # 0 is Non Violent class\n",
        "    return cam1_vid_paths, cam2_vid_paths, label\n",
        "\n",
        "def read_violent_videos() -> List[torch.Tensor] | List[torch.Tensor] | List[int]:\n",
        "    \"\"\"\n",
        "    Read the Violent video paths. Mainly for lazy loading in a later Dataset class\n",
        "\n",
        "    Returns:\n",
        "    cam1_vids: Video paths from camera 1\n",
        "    cam2_vids: Video paths from camera 2\n",
        "    label: a List of the labels (1s in this case, signifying violent)\n",
        "    \"\"\"\n",
        "    violent_cam1_path = \"/content/drive/MyDrive/Datasets/A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/violent/cam1\"\n",
        "    violent_cam2_path = \"/content/drive/MyDrive/Datasets/A-Dataset-for-Automatic-Violence-Detection-in-Videos/violence-detection-dataset/violent/cam2\"\n",
        "    num_violent_videos = 115\n",
        "    cam1_vid_paths = []\n",
        "    cam2_vid_paths = []\n",
        "    for video_num in range(1, num_violent_videos+1):\n",
        "        # Prepare the relative path of the video (for both cameras)\n",
        "        video_index = f\"{video_num}.mp4\"\n",
        "        cam1_video_path = os.path.join(violent_cam1_path,video_index)\n",
        "        cam2_video_path = os.path.join(violent_cam2_path,video_index)\n",
        "\n",
        "        # Append video to list of videos\n",
        "        cam1_vid_paths.append(cam1_video_path)\n",
        "        cam2_vid_paths.append(cam2_video_path)\n",
        "    label = [1]*num_violent_videos*2 # 1 is Violent class\n",
        "    return cam1_vid_paths, cam2_vid_paths, label\n",
        "\n",
        "def read_dataset() -> List[torch.Tensor] | List[torch.Tensor] | List[int]:\n",
        "    \"\"\"\n",
        "    Read the full dataset video paths. Mainly for lazy loading in a later Dataset class\n",
        "\n",
        "    Returns:\n",
        "    cam1_vids: Video paths from camera 1\n",
        "    cam2_vids: Video paths from camera 2\n",
        "    label: a List of the labels. Where each label index corrosponds to a video (first label is the first video's label, and so on...)\n",
        "    \"\"\"\n",
        "    print(\"Loading Non-Violent Dataset...\")\n",
        "    nonv_cam1_vid_paths, nonv_cam2_vid_paths, nonv_label = read_nonViolent_videos()\n",
        "\n",
        "    print(\"Loading Violent Dataset...\")\n",
        "    violent_cam1_vid_paths, violent_cam2_vid_paths, violent_label = read_violent_videos()\n",
        "\n",
        "    print(\"Preparing full dataset...\")\n",
        "    # cam1_video_path = nonv_cam1_vid_paths + violent_cam1_vid_paths\n",
        "    # cam2_vid_paths = nonv_cam2_vid_paths + violent_cam2_vid_paths\n",
        "    # labels = nonv_label + violent_label\n",
        "    # return cam1_video_path, cam2_vid_paths, labels\n",
        "    non_violent_video_paths = nonv_cam1_vid_paths + nonv_cam2_vid_paths\n",
        "    violent_video_paths = violent_cam1_vid_paths + violent_cam2_vid_paths\n",
        "    all_video_paths = non_violent_video_paths + violent_video_paths\n",
        "    labels = nonv_label + violent_label\n",
        "    return all_video_paths, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the dataset"
      ],
      "metadata": {
        "id": "ogtPFxujU_WG"
      },
      "id": "ogtPFxujU_WG"
    },
    {
      "cell_type": "code",
      "source": [
        "# cam1_vid_paths, cam2_vid_paths, labels = read_dataset()\n",
        "all_video_paths, labels = read_dataset()"
      ],
      "metadata": {
        "id": "iMzEq2vOVEOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ccb0e7-c8ec-4b63-82fd-821c30dafe00"
      },
      "id": "iMzEq2vOVEOc",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Non-Violent Dataset...\n",
            "Loading Violent Dataset...\n",
            "Preparing full dataset...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# full_dataset = ViolenceDetectionDataset(cam1_vid_paths=cam1_vid_paths, cam2_vid_paths=cam2_vid_paths, labels=labels, normalize=True)\n",
        "full_dataset = ViolenceDetectionDataset(video_paths=all_video_paths, labels=labels, normalize=True)\n",
        "ds_len = len(labels)\n",
        "train_dataset, test_dataset = random_split(\n",
        "    dataset=full_dataset,\n",
        "    lengths=[round(ds_len*0.8), round(ds_len*0.2)]\n",
        ")\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=2, shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "5sljv7PIapZS"
      },
      "id": "5sljv7PIapZS",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample from the DataLoader object"
      ],
      "metadata": {
        "id": "z2_NqoFZjq4R"
      },
      "id": "z2_NqoFZjq4R"
    },
    {
      "cell_type": "code",
      "source": [
        "X_batch, y_batch = next(iter(test_dataloader))"
      ],
      "metadata": {
        "id": "_bBSThwhjt26"
      },
      "id": "_bBSThwhjt26",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_batch.shape, y_batch.shape"
      ],
      "metadata": {
        "id": "auAnRQMxkyfG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f270dc01-5344-4f29-c1bd-edc2ac94630b"
      },
      "id": "auAnRQMxkyfG",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([2, 3, 16, 112, 112]), torch.Size([2, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model class"
      ],
      "metadata": {
        "id": "84DxLcBFy_M7"
      },
      "id": "84DxLcBFy_M7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### C3D model with FC layers"
      ],
      "metadata": {
        "id": "xlQDonCS2013"
      },
      "id": "xlQDonCS2013"
    },
    {
      "cell_type": "code",
      "source": [
        "class ViolenceDetectionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    FC6 C3D model based on the paper \"Deep Learning for Automatic Violence Detection: Tests on the AIRTLab Dataset\"\n",
        "    P. Sernani, N. Falcionelli, S. Tomassini, P. Contardo and A. F. Dragoni, \"Deep Learning for Automatic Violence Detection: Tests on the AIRTLab Dataset,\" in IEEE Access, vol. 9, pp. 160580-160595, 2021, doi: 10.1109/ACCESS.2021.3131315.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.c3d = nn.Sequential(\n",
        "            nn.Conv3d(in_channels=3, out_channels=64, kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d((1,2,2)),\n",
        "            nn.Conv3d(in_channels=64, out_channels=128, kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d((2,2,2)),\n",
        "            nn.Conv3d(in_channels=128, out_channels=256, kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(in_channels=256, out_channels=256, kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d((2,2,2)),\n",
        "            nn.Conv3d(in_channels=256, out_channels=512, kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(in_channels=512, out_channels=512, kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d((2,2,2)),\n",
        "            nn.Conv3d(in_channels=512, out_channels=512, kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(in_channels=512, out_channels=512, kernel_size=(3,3,3), stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ZeroPad3d((0,1,0,1,0,0)), # (padding_left (Width), padding_right (Width), padding_top (Height), padding_bottom (Height), padding_front (Depth), padding_back (Depth))\n",
        "            nn.MaxPool3d((2,2,2)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=8192, out_features=4096), # The in_features here are set for an frame size of 256, if you'll follow the original paper (112 frame size) go with 8192\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.ann = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=4096, out_features=512),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(in_features=512, out_features=1)\n",
        "        )\n",
        "    def forward(self, X):\n",
        "        X = self.c3d(X)\n",
        "        X = self.ann(X)\n",
        "        return X"
      ],
      "metadata": {
        "id": "Rhf4N7uUbXgA"
      },
      "id": "Rhf4N7uUbXgA",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metrics Functions"
      ],
      "metadata": {
        "id": "fkdND85RsQJm"
      },
      "id": "fkdND85RsQJm"
    },
    {
      "cell_type": "code",
      "source": [
        "def set_metrics(num_classes:int=None, threshold:float=0.5, task:str=\"multiclass\", device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Sets metrics from TorchMetrics\n",
        "\n",
        "    Parameters:\n",
        "    num_classes (int, Default None): The number of unique classes. Leave it if you have a binary class classification task.\n",
        "    threshold (float): The threshold to use for a binary class classification task.\n",
        "    task (str): Whether it's a binary or a multiclass task\n",
        "    device (torch.device or str): the device to use\n",
        "\n",
        "    Returns:\n",
        "    - accuracy_score\n",
        "    - f1_score\n",
        "    - precision\n",
        "    - recall\n",
        "    \"\"\"\n",
        "    if task == \"binary\":\n",
        "        accuracy_score = Accuracy(task=task, threshold=threshold).to(device)\n",
        "        f1_score = F1Score(task=task, threshold=threshold).to(device)\n",
        "        precision = Precision(task=task, threshold=threshold).to(device)\n",
        "        recall = Recall(task=task, threshold=threshold).to(device)\n",
        "    else:\n",
        "        if num_classes is None:\n",
        "            raise ValueError(\"num_classes cannot be `None` for a multiclass classification task\")\n",
        "        accuracy_score = Accuracy(task=task, num_classes=num_classes, average='macro').to(device)\n",
        "        f1_score = F1Score(task=task, num_classes=num_classes, average='macro').to(device)\n",
        "        precision = Precision(task=task, num_classes=num_classes, average='macro').to(device)\n",
        "        recall = Recall(task=task, num_classes=num_classes, average='macro').to(device)\n",
        "    return accuracy_score, f1_score, precision, recall\n",
        "\n",
        "def classification_report(num_classes:int, y_true, y_pred, task=\"multiclass\", device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Creates a full classification report.\n",
        "\n",
        "    Parameters:\n",
        "    - num_classes (int): The number of unique classes.\n",
        "    - y_true (torch.Tensor): The ground truth values.\n",
        "    - y_pred (torch.Tensor): The predicted values.\n",
        "    - task (str): binary or multiclass task.\n",
        "    - device (torch.device or str): the device to use.\n",
        "\n",
        "    Returns:\n",
        "    - report (str)\n",
        "    \"\"\"\n",
        "    accuracy_score, f1_score, precision, recall = set_metrics(num_classes, task=task, device=device)\n",
        "    report_dict = {\n",
        "        \"Accuracy\":     accuracy_score(y_pred, y_true).item(),\n",
        "        \"Precision\":    precision(y_pred, y_true).item(),\n",
        "        \"Recall\":       recall(y_pred, y_true).item(),\n",
        "        \"F1-Score\":     f1_score(y_pred, y_true).item()\n",
        "        }\n",
        "\n",
        "    report = f\"\"\"Full Classification Report:\n",
        "        - Accuracy:  {report_dict[\"Accuracy\"]}\n",
        "        - Precision: {report_dict[\"Precision\"]}\n",
        "        - Recall:    {report_dict[\"Recall\"]}\n",
        "        - F1-Score:  {report_dict[\"F1-Score\"]}\n",
        "    \"\"\"\n",
        "    return report"
      ],
      "metadata": {
        "id": "Q2nkksMMsRm0"
      },
      "id": "Q2nkksMMsRm0",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training functions"
      ],
      "metadata": {
        "id": "RHATGQNKrtNI"
      },
      "id": "RHATGQNKrtNI"
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"binary\"\n",
        "\n",
        "def training_step(loss_fn:torch.nn.modules.loss, optimizer:torch.optim, model:torch.nn.Module, X:torch.Tensor, y:torch.Tensor):\n",
        "    \"\"\"\n",
        "    A single training step, used for testing purposes and inside of bigger training loops functions. made with PyTorch in mind.\n",
        "\n",
        "    Paramters:\n",
        "    - loss_fn(torch.nn.modules.loss): The loss function to use\n",
        "    - optimizer (torch.optim): The optimizer to use\n",
        "    - model (torch.nn.Module): The model to train\n",
        "    - X (torch.Tensor): The features matrix\n",
        "    - y (torch.Tensor): The target variable\n",
        "\n",
        "    Returns:\n",
        "    - Loss\n",
        "    - Logits\n",
        "    \"\"\"\n",
        "    logits = model(X)\n",
        "    loss = loss_fn(logits, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item(), logits\n",
        "\n",
        "def testing_step(loss_fn:torch.nn.modules.loss, model:torch.nn.Module, X:torch.Tensor, y:torch.Tensor):\n",
        "    \"\"\"\n",
        "    A single inference step, used for testing purposes and inside of bigger testing/inferece loops functions. made with PyTorch in mind.\n",
        "\n",
        "    Paramters:\n",
        "    - loss_fn(torch.nn.modules.loss): The loss function to use\n",
        "    - model (torch.nn.Module): The model to use\n",
        "    - X (torch.Tensor): The features matrix\n",
        "    - y (torch.Tensor): The target variable\n",
        "\n",
        "    Returns:\n",
        "    - Loss\n",
        "    - Logits\n",
        "    \"\"\"\n",
        "    with torch.inference_mode():\n",
        "        logits = model(X)\n",
        "        loss = loss_fn(logits, y)\n",
        "    return loss.item(), logits\n",
        "\n",
        "def test_loop(loss_fn:torch.nn.modules.loss, model:torch.nn.Module, test_dataloader:torch.utils.data.DataLoader, device):\n",
        "    \"\"\"\n",
        "    Inference loop, goes through a full DataLoader object and runs inference on it. Designed with PyTorch in mind\n",
        "\n",
        "    Parameters:\n",
        "    - loss_fn(torch.nn.modules.loss): The loss function to use\n",
        "    - model (torch.nn.Module): The model to use\n",
        "    - test_dataloader (torch.utils.data.DataLoader): The test dataloader\n",
        "    - device (torch.device or str): which device to use\n",
        "\n",
        "    Returns:\n",
        "    - test_loss: The average testing loss across all batches\n",
        "    - test_acc: The average testing accuracy across all batches\n",
        "    - test_f1: The average testing F1 score across all batches\n",
        "    \"\"\"\n",
        "    accuracy_score, f1_score, precision, recall = set_metrics(task=task, device=device)\n",
        "    test_loss = 0\n",
        "    model.eval()\n",
        "    for X_test_batch, y_test_batch in test_dataloader:\n",
        "        X_test_batch, y_test_batch = X_test_batch.to(device), y_test_batch.to(device)\n",
        "        sample_test_loss, y_pred = testing_step(\n",
        "            loss_fn, model, X_test_batch, y_test_batch\n",
        "        )\n",
        "        accuracy_score.update(torch.sigmoid(y_pred), y_test_batch)\n",
        "        f1_score.update(torch.sigmoid(y_pred), y_test_batch)\n",
        "        test_loss += sample_test_loss\n",
        "    test_acc = accuracy_score.compute()\n",
        "    test_f1 = f1_score.compute()\n",
        "    accuracy_score.reset()\n",
        "    f1_score.reset()\n",
        "    test_loss /= len(test_dataloader)\n",
        "    return test_loss, test_acc, test_f1\n",
        "\n",
        "def train_loop(loss_fn:torch.nn.modules.loss, optimizer:torch.optim, model:torch.nn.Module, train_dataloader:torch.utils.data.DataLoader, device, lr_scheduler:torch.optim.lr_scheduler=None):\n",
        "    \"\"\"\n",
        "    Training loop, goes through a full DataLoader object and trains on it. Designed with PyTorch in mind\n",
        "\n",
        "    Parameters:\n",
        "    - loss_fn(torch.nn.modules.loss): The loss function to use\n",
        "    - optimizer (torch.optim): The optimizer to use\n",
        "    - model (torch.nn.Module): The model to train\n",
        "    - train_dataloader (torch.utils.data.DataLoader): The train dataloader\n",
        "    - lr_scheduler (torch.optim.lr_scheduler or None): The Learning Rate Scheduler to use\n",
        "    - device (torch.device or str): which device to use\n",
        "\n",
        "    Returns:\n",
        "    - train_loss: The average training loss across all batches\n",
        "    - train_acc: The average training accuracy across all batches\n",
        "    - train_f1: The average training F1 score across all batches\n",
        "    \"\"\"\n",
        "    accuracy_score, f1_score, precision, recall = set_metrics(task=task, device=device)\n",
        "    train_loss = 0\n",
        "    model.train()\n",
        "    for X_batch_train, y_batch_train in train_dataloader:\n",
        "        X_batch_train, y_batch_train = (\n",
        "            X_batch_train.to(device),\n",
        "            y_batch_train.to(device),\n",
        "        )\n",
        "        sample_train_loss, y_pred = training_step(\n",
        "            loss_fn, optimizer, model, X_batch_train, y_batch_train\n",
        "        )\n",
        "        train_loss += sample_train_loss\n",
        "        accuracy_score.update(torch.sigmoid(y_pred), y_batch_train)\n",
        "        f1_score.update(torch.sigmoid(y_pred), y_batch_train)\n",
        "    if lr_scheduler is not None:\n",
        "        lr_scheduler.step()\n",
        "    train_acc = accuracy_score.compute()\n",
        "    train_f1 = f1_score.compute()\n",
        "    accuracy_score.reset()\n",
        "    f1_score.reset()\n",
        "    train_loss /= len(train_dataloader)\n",
        "    return train_loss, train_acc, train_f1\n",
        "\n",
        "def train_test_loop(\n",
        "    epochs: int, loss_fn:torch.nn.modules.loss, optimizer:torch.optim, model:torch.nn.Module, train_dataloader:torch.utils.data.DataLoader, test_dataloader:torch.utils.data.DataLoader, device, lr_scheduler=None, early_stopping=None, verbose=True,\n",
        "    ):\n",
        "    \"\"\"\n",
        "    A full train-test loop, where it trains on a given train_dataloader and runs inference using the model on a test_dataloader, for a given number of epochs.\n",
        "\n",
        "    Parameters:\n",
        "    - epochs (int): The number of epochs to run\n",
        "    - loss_fn(torch.nn.modules.loss): The loss function to use\n",
        "    - optimizer (torch.optim): The optimizer to use\n",
        "    - model (torch.nn.Module): The model to train\n",
        "    - train_dataloader (torch.utils.data.DataLoader): The train dataloader\n",
        "    - test_dataloader (torch.utils.data.DataLoader): The test dataloader\n",
        "    - device (torch.device or str): the device to use (cuda or cpu)\n",
        "    - lr_scheduler (torch.optim.lr_scheduler or None): The Learning Rate Scheduler to use\n",
        "    - early_stopping (custom class or None): The early stopping class to use\n",
        "    - verbose (bool, default True): Whether to print more information or not\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc, train_f1 = train_loop(\n",
        "            loss_fn=loss_fn, optimizer=optimizer, model=model, train_dataloader=train_dataloader, lr_scheduler=lr_scheduler, device=device\n",
        "        )\n",
        "        test_loss, test_acc, test_f1 = test_loop(loss_fn=loss_fn, model=model, test_dataloader=test_dataloader, device=device)\n",
        "        print(\n",
        "            f\"\"\"[INFO] Epoch #{epoch+1}\\nTraining Loss = {train_loss:.2f} | Testing Loss = {test_loss:.2f}\n",
        "Training Accuracy = {train_acc:.2%} | Testing Accuracy = {test_acc:.2%}\n",
        "Training F1-Score = {train_f1:.2%} | Testing F1-Score = {test_f1:.2%}\n",
        "--------------------------------------------------------\"\"\"\n",
        "        ) if verbose else None\n",
        "\n",
        "        writer.add_scalars(\n",
        "            \"Train Loss vs Test Loss across all batches\",\n",
        "            {\"Train Loss\": train_loss, \"Test Loss\": test_loss},\n",
        "            epoch + 1,\n",
        "        )\n",
        "        writer.add_scalars(\n",
        "            \"Train Accuracy vs Test Accuracy across all batches\",\n",
        "            {\"Train Accuracy\": train_acc, \"Test Accuracy\": test_acc},\n",
        "            epoch + 1,\n",
        "        )\n",
        "        if early_stopping is not None:\n",
        "            early_stopping(test_loss, model)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                writer.close()\n",
        "                break\n",
        "    writer.close()\n",
        "    print(\"Done Training!\")"
      ],
      "metadata": {
        "id": "h0pKmqckrvqg"
      },
      "id": "h0pKmqckrvqg",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the model"
      ],
      "metadata": {
        "id": "mvXfq3D7tQlj"
      },
      "id": "mvXfq3D7tQlj"
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViolenceDetectionModel()\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "lr=3e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "8NONy7bKtLod"
      },
      "id": "8NONy7bKtLod",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "5a5x4_HHSAWU"
      },
      "id": "5a5x4_HHSAWU"
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "epochs = 5\n",
        "train_test_loop(\n",
        "    epochs=epochs,\n",
        "    loss_fn=loss_fn, optimizer=optimizer, model=model,\n",
        "    train_dataloader=train_dataloader, test_dataloader=test_dataloader,\n",
        "    device=device,\n",
        "    lr_scheduler=None, early_stopping=None,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "jw_q1rmJuEtz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500,
          "referenced_widgets": [
            "9f9da63460854013bd03e4050975722e",
            "86d00d1007864ea7a229bd647445e014",
            "2c914fb81b4e4f08b881420a27e77aea",
            "0c187eebbf9b4b69aafc695eeef067b9",
            "eefc9f58cac04bf3a96adcad1337918a",
            "efd361c8b37e462981af7270b5a01a67",
            "790260036a5a48b9bcf4266de5eb2981",
            "68a8cbb7035749c688f29cba9ac4d341",
            "5d3b8e0ca3ce4130a4f932870b278092",
            "df4b28ed74cf482d9e4191dbd23624cb",
            "07d8e264dd95401797043c7fcf0ca76c"
          ]
        },
        "outputId": "03cbd731-98c1-450b-ee37-4231d488ed7b"
      },
      "id": "jw_q1rmJuEtz",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f9da63460854013bd03e4050975722e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Epoch #1\n",
            "Training Loss = 715.91 | Testing Loss = 0.67\n",
            "Training Accuracy = 65.00% | Testing Accuracy = 61.43%\n",
            "Training F1-Score = 78.41% | Testing F1-Score = 76.11%\n",
            "--------------------------------------------------------\n",
            "[INFO] Epoch #2\n",
            "Training Loss = 0.65 | Testing Loss = 0.68\n",
            "Training Accuracy = 66.07% | Testing Accuracy = 61.43%\n",
            "Training F1-Score = 79.30% | Testing F1-Score = 76.11%\n",
            "--------------------------------------------------------\n",
            "[INFO] Epoch #3\n",
            "Training Loss = 0.65 | Testing Loss = 0.67\n",
            "Training Accuracy = 66.07% | Testing Accuracy = 61.43%\n",
            "Training F1-Score = 79.57% | Testing F1-Score = 76.11%\n",
            "--------------------------------------------------------\n",
            "[INFO] Epoch #4\n",
            "Training Loss = 0.65 | Testing Loss = 0.67\n",
            "Training Accuracy = 67.14% | Testing Accuracy = 61.43%\n",
            "Training F1-Score = 80.00% | Testing F1-Score = 76.11%\n",
            "--------------------------------------------------------\n",
            "[INFO] Epoch #5\n",
            "Training Loss = 0.66 | Testing Loss = 0.70\n",
            "Training Accuracy = 66.07% | Testing Accuracy = 38.57%\n",
            "Training F1-Score = 79.21% | Testing F1-Score = 0.00%\n",
            "--------------------------------------------------------\n",
            "Done Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D144p7FWX-Sq"
      },
      "id": "D144p7FWX-Sq",
      "execution_count": 16,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "001c797b",
        "S8ulY2jpknGT",
        "a0d77056",
        "4e7bdf10",
        "ogtPFxujU_WG",
        "z2_NqoFZjq4R",
        "xlQDonCS2013",
        "fkdND85RsQJm",
        "RHATGQNKrtNI"
      ],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f9da63460854013bd03e4050975722e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86d00d1007864ea7a229bd647445e014",
              "IPY_MODEL_2c914fb81b4e4f08b881420a27e77aea",
              "IPY_MODEL_0c187eebbf9b4b69aafc695eeef067b9"
            ],
            "layout": "IPY_MODEL_eefc9f58cac04bf3a96adcad1337918a"
          }
        },
        "86d00d1007864ea7a229bd647445e014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efd361c8b37e462981af7270b5a01a67",
            "placeholder": "​",
            "style": "IPY_MODEL_790260036a5a48b9bcf4266de5eb2981",
            "value": "100%"
          }
        },
        "2c914fb81b4e4f08b881420a27e77aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68a8cbb7035749c688f29cba9ac4d341",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d3b8e0ca3ce4130a4f932870b278092",
            "value": 5
          }
        },
        "0c187eebbf9b4b69aafc695eeef067b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df4b28ed74cf482d9e4191dbd23624cb",
            "placeholder": "​",
            "style": "IPY_MODEL_07d8e264dd95401797043c7fcf0ca76c",
            "value": " 5/5 [2:07:51&lt;00:00, 1503.96s/it]"
          }
        },
        "eefc9f58cac04bf3a96adcad1337918a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd361c8b37e462981af7270b5a01a67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "790260036a5a48b9bcf4266de5eb2981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68a8cbb7035749c688f29cba9ac4d341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d3b8e0ca3ce4130a4f932870b278092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df4b28ed74cf482d9e4191dbd23624cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07d8e264dd95401797043c7fcf0ca76c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}